%!TEX root = main.tex
% UTF-8 encoding
\section{Introduction}
\label{sec:intro}

In recent years,
large social media companies have been hiring small armies of ``content moderators''
to prevent conversations on their platforms 
that they deem to be inappropriate.
The hours are long, 
the pay low, 
and the firehose of humanity's worst impulses, endless.
In 2019, The Verge reported on the emotional toll this work exacts,
leading in some cases
to post-traumatic stress disorder~\cite{newton2019terror, newton2019trauma}.

Automation is an obvious way to assist content moderators.
Ideally, they would be able to make a decision once
and have it applied consistently to all similar content.
One standard form of automated moderation is ``ban-lists'' of forbidden words.
These are easy to implement, and define a clear-cut policy.
However, they are also easy to evade:
as soon as terms are added to a ban-list,
the offenders will notice
and adapt by inventing euphemisms to evade the filters~\cite{ofcom:ai2019}.
Euphemisms are frequently words with other, innocuous meanings
so they cannot be filtered unconditionally;
they must be interpreted in context.
To illustrate the problem,
Table~\ref{table:example2} gives many examples of euphemisms
for a few terms that are frequently forbidden.
Almost all of the euphemisms have innocuous meanings.
Table~\ref{table:example1} shows how a few of the euphemisms
would be used in context, demonstrating that
a human reader can often tell that a euphemistic meaning is intended
even if they do not know exactly what the meaning is.

We present techniques for automated assistance
with two tasks related to ban-list maintenance.
Our algorithm for \textbf{euphemism detection}
takes as input a set of \textit{target keywords} referring to forbidden topics
and produces a set of \textit{candidate euphemisms}
that may signify the same concept as one of the target keywords,
without identifying which one.
\textbf{Euphemism identification} takes a single euphemism as input
and identifies its meaning.
We envision these algorithms being used in a pipeline
where moderators apply both in succession
to detect new euphemisms and understand their meaning.
For instance, if the target keywords are formal drug names 
(\eg, marijuana, heroin, cocaine),
euphemism detection might find common slang names for these drugs
(\eg, pot, coke, blow, dope)
and euphemism identification could then associate each euphemism
with the corresponding formal name
(\eg, $\text{pot} \longrightarrow \text{marijuana}$,
$\text{coke, blow} \longrightarrow \text{cocaine}$,
$\text{dope} \longrightarrow \text{heroin}$).

In addition to their practical use in content moderation,
our algorithms advance the state of the art in natural language processing (NLP)
by demonstrating the feasibility of self-supervised learning
to process large corpora of unstructured, non-canonical text
(\eg, underground forum posts),
a challenging task of independent interest to the NLP community
(\eg, \cite{durrett2017identifying,portnoff2017tools,felbo2017using}).
Our algorithms require no manual annotation of text
or parameter tuning,
and do not just rely on a `black box'  pre-trained and
fine-tuned model.

\begin{table*}[t!]
	\centering
	\small
	\caption{Examples of the variety of euphemisms associated with target keywords in commonly forbidden categories.}
	\begin{tabular}{lll}
		\toprule
		\multicolumn{1}{c}{\textbf{Category}} & \multicolumn{1}{c}{\textbf{Target Keyword}} & \multicolumn{1}{c}{\textbf{Euphemisms}} \\
		\midrule
                                 & Marijuana    & blue jeans, blueberry, grass, gold, green, kush, popcorn, pot, root, shrimp, smoke, sweet lucy, weed \\
                \textbf{Drugs}   & Amphetamines & clear, dunk, gifts, girls, glass, ice, nails, one pot, shaved ice, shiny girl, yellow cake \\
                                 & Heroin       & avocado, bad seed, ballot, beast, big H,  cheese, chip, downtown, hard candy, mexican horse, pants \\
                \addlinespace
                \textbf{Weapons} & Gun          & bap, boom stick, burner, chopper, cuete, gat, gatt, hardware, heater, mac, nine, piece, roscoe, strap \\
                                 & Bullet       & ammo, cap, cop killer, lead, rounds \\
                \addlinespace
                \textbf{Sex}     & Breasts      & bazooms, boobs, lungs, na-nas, puppies, tits, yabo \\
                                 & Prostitution & call girl, girlfriend experience, hooker, poon, whore, working girl \\
    \bottomrule
	\end{tabular}
	\label{table:example2}
\end{table*}

\begin{table*}[ht!]
	\centering
	\small
	\caption{Example usage for a few of the euphemisms in Table~\ref{table:example2}.}
	\begin{tabular}{l@{\hspace{\interwordspace}}l@{\qquad}l}
		\toprule
		& \multicolumn{1}{c}{\textbf{Example Sentence} (euphemism in boldface)}
		& \multicolumn{1}{c}{\textbf{Euphemism means}} \\
		\midrule
		1. & I had to shut up: the dealers had \textbf{gats}, my boys didn't.
		   & \textit{machine pistol} \\\addlinespace[3pt]
		2. & For all vendors of \textbf{ice},
		     it seems pretty obvious that it is not as pure as they market it.
		   & \textit{methamphetamine} \\\addlinespace[3pt]
		3. & I feel really good and warm behind the eyes.
		     It's not something I've felt before on \textbf{pot} alone to this degree.
		   & \textit{marijuana} \\\addlinespace[3pt]
		4. & You can get an ounce of this \textbf{blueberry kush}
		     for like \$300 and it's insane.
		   & \textit{variety of marijuana} \\\addlinespace[3pt]
		5. & I'm looking for the \textbf{girlfriend experience},
		     without having to deal with an actual girlfriend.
		   & \textit{form of prostitution} \\
		\bottomrule
	\end{tabular}
	\label{table:example1}
\end{table*}

\subsection{Euphemism Detection}
\begin{table}[ht]
	\centering
	\small
	\caption{Examples of informative and uninformative contexts.  The word ``heroin'' has been masked out of each sentence below; in cases 1--3 it is clear that the masked word must be the name of an addictive drug, while in cases 4--6 there are more possibilities.}
	    \newcommand{\maskedword}{\rule{3em}{1.5ex}}
		\begin{tabular}{ll@{\hspace{\interwordspace}}p{0.65\columnwidth}}
\toprule
		\multicolumn{1}{c}{\textbf{Context}} &  \multicolumn{2}{c}{\textbf{Example Sentences}} \\
		\midrule
		\textbf{Informative}
		& 1. & This 22 year old former \maskedword{} addict who I did drugs with was caught this night. \\
		& 2. & I have xanax real roxi opana cole and \maskedword{} for sale. \\
		& 3. & Six \maskedword{} overdoses in seven hours in wooster two on life support. \\\addlinespace
		\textbf{Uninformative}
		& 4. & Why is it so hard to find \maskedword{}? \\
		& 5. & The quality of this \maskedword{} is amazing and for the price its unbelievable. \\
		& 6. & Could we in the future see \maskedword{} shampoo? \\
		\bottomrule
	\end{tabular}
	\label{table:example3}
\end{table}

%\noindent \textbf{Euphemism Detection}: 
The main challenge of automated euphemism detection
is distinguishing the euphemistic meaning of a term
from its innocuous ``cover'' meaning~\cite{yuan2018reading}.
For example, in sentence 2 of Table~\ref{table:example1},
``ice'' \emph{could} refer to frozen water. 
To human readers, this is unlikely in context,
because the purity of frozen water
is usually not a concern for purchasers.
Previous attempts to automate this task~\cite{takuro2020codewords,magu2018determining,yuan2018reading,zhao2016chinese}
relied on \emph{static word embeddings} (\eg, word2vec~\cite{mikolov2013efficient}).
While these make implicit use of contextual information (availing the benefit of the distributional hypothesis that semantically similar words occur in linguistically similar contexts), 
they do not distinguish different senses of the same word.
They performed poorly;
continuing the ``ice'' example,
sentences using it in its frozen-water sense
would crowd out the sentences using it as an euphemism
and prevent the discovery of the euphemistic meaning.

A newer class of \emph{context-aware} embeddings (\eg BERT~\cite{devlin2019bert})
learns a different word representation
for every context in which the word appears,
so they do not conflate different senses of the same word.
However, using context-aware embeddings brings its own problems.
The similarity of two words is no longer well-defined,
since there are now several vectors associated with each word.
This means context-aware embeddings cannot be substituted
for the static embeddings used in earlier euphemism detection papers,
which relied on word similarity comparisons.
Also, not all contexts are equal.
For any given term,
some sentences that use it
will encode more information about its meaning
than others do.
Table~\ref{table:example3} illustrates the problem:
it is easier to deduce what the masked term probably was
in sentences 1--3
than sentences 4--6.
This can be addressed by manually labeling sentences
as informative or uninformative,
but our goal is to develop an algorithm that needs no manual labels.

%For a given euphemism, some sentences containing the euphemism encode more information about the term's meaning than others. 
%Table \ref{table:example3} lists sentences with \emph{uninformative} contexts (cases 4--6).
%Hence, to utilize context-aware embeddings, we need to separate informative contexts from uninformative ones, 
%which is challenging without prior labels. %https://www.overleaf.com/project/5faab814f2e47b53a29de6f1
%
%A first approach for solving these problems would be to replace the static embeddings from prior approaches with a recently-proposed class of \emph{context-aware embeddings} (\eg, ) that 
%This approach presents two main challenges: 
%(1) prior approaches rely on computing distances between word embeddings, which are not well-defined for context-aware embeddings.
%Natural options for solving this issue (\eg, averaging a word's embedding over all contexts) negate the benefits of context-aware embeddings.  
%(2) Not all contexts are equal. 
%That is, 
%% , if we remove the drug words from those sentences, the remaining context contains limited information.
% Based on the observation that contexual information is distinct enough for a human expert to make the decision, we formulate the problem as an unsupervised fill-in-the-mask problem and solve it by a masked language model with BERT. 



In this paper, we design an end-to-end pipeline for detecting euphemisms by making \textit{explicit} use of context. 
This is particularly important to help content moderation of text in forums. 
We formulate the problem as an unsupervised fill-in-the-mask problem \cite{devlin2019bert,donahue2020enabling} and solve it by combining a masked language model (\eg, proposed in BERT \cite{devlin2019bert}) with a novel self-supervised algorithm to filter out uninformative contexts.
% Specifically, we obtain the contexual information of target keywords by getting their surrounding words, filtering the generic contextual information which brings more noise than useful context (An example is shown in Table \ref{table:example3}), and generating the euphemism candidates by a Masked Language Model (MLM) proposed in BERT. 
The salience of our approach, which sets itself apart from other work on euphemism detection, lies in its non-reliance on linguistic resources (\eg, a sentiment lexicon) \cite{felt2020recognizing}, search-engine results, or a seed set   of euphemisms. 
As such it is particularly relevant to our application case---online
platforms with free-flowing discourse that may adopt their own 
vernacular over time. 
Evaluating on a variety of representative datasets of online posts
we found that our approach yields  top-$k$ detection accuracies that are 30--400\% higher than state-of-the-art baseline approaches on all of the datasets, with top-20 accuracies as high as 40--50\%, which is high for this problem. 
A qualitative analysis reveals that our approach also discovers correct euphemisms that {\em were not on our ground truth lists}, \ie, it can 
detect previously unknown euphemisms. 
Again, this is highly valuable in the context of 
Internet communities, where memes 
and slang lead to rapidly evolving vocabulary.

\subsection{Euphemism Identification}
\label{sec:intro_iden}
%\noindent \textbf{Euphemism Identification}: 
\begin{table*}[t!]
	\centering
	\small
	\caption{Representative sentences from Reddit with euphemisms. Cases 1--3 show euphemisms (``coke,'' ``pot'') are used in the euphemistic sense (\ie, the drug sense) while cases 4--6 show that the candidate euphemism is used in non-euphemistic senses.}
	\begin{tabular}{p{0.1\textwidth}p{0.8\textwidth}}
		\toprule
		\multicolumn{1}{c}{\textbf{Euphemism}} &  \multicolumn{1}{c}{\textbf{Example sentences}} \\
		\midrule
		\multirow{7}{*}{\centering \textbf{\space \space \space \space Coke}}
		& 1. We had already paid \$70 for some shitty weed from a taxi driver but we were interested in some coke and the cubans. \\
		& 2. Why are coke dealers the most nuttiest? \\
		& 3. OK so we have one gram high quality coke between 2 people who have never done more than a bump. \\
		& 4. I love having coke with ice. \\
		& 5. When I buy coke at the beverage shop in UK, I pay neither a transaction fee nor an exchange fee. \\
		& 6. Never have tried mixing coke with sprite or 7up. \\ 
		\midrule
		\multirow{7}{*}{\centering \textbf{\space \space \space \space Pot}}
		& 1. My cousin did the same and when the legalized pot in dc they really started cracking down in virginia and maryland. \\
		& 2. As far as we know he was still smoking pot but that was it. \\
		& 3. Age 17, every time I smoked pot, I felt out of place. \\
		& 4. No one would resist a pot of soup. \\
		& 5. There's plenty of cupboard space in the kitchen for all your pots and pans. \\
		& 6. Most lilies grow well in pots. \\
		\bottomrule
	\end{tabular}
	\label{table:example4}
\end{table*}

Once the usage of euphemisms has been detected, it is important to \emph{identify} what each euphemism refers to. Unlike the  task of deciding whether a given word refers to \textit{any} target keyword (euphemism detection), the task of euphemism identification maps a given euphemism to a \textit{specific} target keyword. This involves not only using the nuance of contextual information but also aggregating this information from related instances across the collection to make the inference.     
Again, referring to the 2nd and 3rd examples in Table \ref{table:example1}, we want to identify that {\em ice} refers to {\em methamphetamine} and {\em pot}  to {\em marijuana}. 
To the best of our knowledge, no prior work has explicitly captured the meaning of a euphemism except for a few peripheral works (\eg, \cite{yuan2018reading}) that identify the broad category of a euphemism (\eg, sedative, narcotic, or stimulant for a drug euphemism). 

Euphemism identification poses four main challenges: 

1) The distinction in meaning between the target keywords  (\eg, cocaine and marijuana) is often subtle and difficult  to learn from  raw text corpora alone. 
2) A given euphemism can  be used in a euphemistic or  non-euphemistic sense, adding  the extra layer of linguistic nuance (Table \ref{table:example4}). 
3) No curated datasets that are publicly available are adequate to exhaustively learn a growing list of mappings between euphemisms and their target keywords.
4) It is unclear what linguistic and ontological resources one would need to automate this task. 

In this paper, we propose the first approach to identify the precise meaning of a euphemism (\eg, mapping {\em pot}  to {\em marijuana} and {\em Adam} to {\em ecstasy}). 
We systematically address  the  challenges identified above via a self-supervised learning scheme, a classification formulation, and a coarse-to-fine-grained framework. 
The key novelty lies in how we formulate the problem and solve it without additional resources or supervision. 
Going beyond demonstrating the feasibility of the task on a variety of datasets, we observed improvements in top-$k$ accuracy between 25--80\%  compared to constructed baseline approaches. 



% \subsection{Contributions}
% To summarize, we make the following contributions: 
% \begin{itemize}%[leftmargin=*]
% 	\item We identify the most important information (\ie, contextual information) in the task of euphemism detection, formulate the problem to a fill-in-the-mask problem, and propose to use a masked language model to solve it in an unsupervised way. 
% 	\item We make the first attempt to the task of euphemism identification. To solve the resource challenge, the linguistic challenge and the noise challenge aforementioned, we utilize a self-supervised learning scheme and propose a coarse-to-fine-grained classification approach. 
% 	\item We perform extensive experiments on three representative datasets. The results demonstrate that our proposed approach significantly outperforms the baseline methods. 
% \end{itemize}

